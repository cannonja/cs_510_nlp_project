{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install gensim sklearn numpy pandas\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from gensim.sklearn_api import D2VTransformer, W2VTransformer\n",
    "from sklearn.base import BaseEstimator, MetaEstimatorMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Set to None to run entire dataset. Set to an integer to limit dataset size.\n",
    "DATASET_LIMIT = 500\n",
    "\n",
    "# Uncomment dataset to train/test\n",
    "DATASET_FILE = 'IMDB_Dataset.csv'\n",
    "# DATASET_FILE = 'SST_dataset.csv'\n",
    "\n",
    "# File containing EWE vectors\n",
    "EWE_FILE = 'ewe_uni.txt'\n",
    "\n",
    "def load_ewe(file):\n",
    "  \"\"\"Loads EWE data from file and returns a dictionary of words with\n",
    "  corresponding EWE vectors. Words with 0.0 vectors are filtered out.\"\"\"\n",
    "  vectors = {}\n",
    "  good_count = 0\n",
    "  bad_count = 0\n",
    "  with open(file, 'r') as data_file:\n",
    "    for line in data_file.readlines():\n",
    "      # Break line into items\n",
    "      items = line.split(' ')\n",
    "      # First item is the word\n",
    "      word = items[0]\n",
    "      # Next 300 items are the word's vectors\n",
    "      rest = items[1:]\n",
    "      assert len(rest) == 300\n",
    "      vecs = []\n",
    "      zeros = False\n",
    "      for v in rest:\n",
    "        vec = float(v)\n",
    "        # Check if any zeros are present\n",
    "        if vec == 0:\n",
    "          zeros = True\n",
    "        vecs.append(vec)\n",
    "      # If values look valid, add to dictionary\n",
    "      if zeros == False:\n",
    "        vectors[word] = vecs\n",
    "        good_count += 1\n",
    "        if good_count % 10000 == 0 and good_count != 0:\n",
    "          print(\"EWE: Loaded\", good_count, \"words.\")\n",
    "      # If values have zeros, ignore word\n",
    "      else:\n",
    "        bad_count += 1\n",
    "  print(\"EWE: Loaded\", good_count, \"words.\")\n",
    "  print(\"EWE: Ignored\", bad_count, \"words with zero-length vectors.\")\n",
    "  return vectors\n",
    "\n",
    "class WordTokenizer(BaseEstimator, MetaEstimatorMixin):\n",
    "  \"\"\"Tokenize input strings based on a simple word-boundary pattern.\n",
    "  This seems to be required in the pipeline for EWE and W2V Transformer\n",
    "  to operate.\"\"\"\n",
    "  \n",
    "  def fit(self, X, y=None):\n",
    "    return self\n",
    "    \n",
    "  def transform(self, X):\n",
    "    token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "    parser = lambda doc: token_pattern.findall(doc)\n",
    "    X = [parser(x) for x in X]\n",
    "    return X\n",
    "\n",
    "class EWETransformerDocLevel(W2VTransformer):\n",
    "  \"\"\"A modified Word2Vec sklearn-wrapper class to apply EWE vectors.\n",
    "  Averages the words in each document. Based on the W2VTransformerDocLevel\n",
    "  class below.\"\"\"\n",
    "    \n",
    "  def __init__(self, size=300, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=1e-3, seed=1,\n",
    "    workers=4, min_alpha=0.0001, sg=0, hs=0, negative=5, cbow_mean=1, hashfxn=hash, iter=5, null_word=1,\n",
    "    trim_rule=None, sorted_vocab=1, batch_words=10000):\n",
    "    super().__init__(size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words)\n",
    "\n",
    "  def transform(self, docs):\n",
    "    \n",
    "    doc_vecs = []\n",
    "    #print(len(docs), \"docs\")\n",
    "    for doc in docs:\n",
    "      # for each document generate a word matrix\n",
    "      word_vectors_per_doc = []\n",
    "      for word in doc:\n",
    "        word_lower = word.lower()\n",
    "        # handle out-of vocabulary words\n",
    "        if word_lower in ewe_vectors:\n",
    "          word_vectors_per_doc.append(ewe_vectors[word_lower])\n",
    "      if len(word_vectors_per_doc) == 0:\n",
    "        print(\"Zero word vectors found for:\")\n",
    "        print(doc)\n",
    "      word_vectors_per_doc = np.array(word_vectors_per_doc)\n",
    "      if len(word_vectors_per_doc) == 0:\n",
    "        print(\"Zero word vectors after numpification found for:\")\n",
    "        print(doc)\n",
    "      # take the column-wise mean of this matrix and store\n",
    "      mean = word_vectors_per_doc.mean(axis=0)\n",
    "      doc_vecs.append(mean)\n",
    "    array = np.array(doc_vecs)\n",
    "    #print(\"array shape:\", array.shape)\n",
    "    return array\n",
    "\n",
    "class W2VTransformerDocLevel(W2VTransformer):\n",
    "  \"\"\"Extends Gensim's Word2Vec sklearn-wrapper class to further transform\n",
    "  word-vectors into doc-vectors by averaging the words in each document.\n",
    "  Suggestion for W2V integration taken from following page:\n",
    "  https://github.com/alex2awesome/gensim-sklearn-tutorial/blob/master/notebooks/gensim-in-sklearn-pipelines.ipynb\"\"\"\n",
    "    \n",
    "  def __init__(self, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=1e-3, seed=1,\n",
    "    workers=4, min_alpha=0.0001, sg=0, hs=0, negative=5, cbow_mean=1, hashfxn=hash, iter=5, null_word=0,\n",
    "    trim_rule=None, sorted_vocab=1, batch_words=10000):\n",
    "    super().__init__(size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words)\n",
    "    \n",
    "  def transform(self, docs):      \n",
    "    doc_vecs = []\n",
    "    for doc in docs:\n",
    "      # for each document generate a word matrix\n",
    "      word_vectors_per_doc = []\n",
    "      for word in doc:\n",
    "        # handle out-of vocabulary words\n",
    "        word_lower = word.lower()\n",
    "        if word_lower in self.gensim_model.wv:\n",
    "          word_vectors_per_doc.append(self.gensim_model.wv[word_lower])            \n",
    "      word_vectors_per_doc = np.array(word_vectors_per_doc)\n",
    "      # take the column-wise mean of this matrix and store\n",
    "      mean = word_vectors_per_doc.mean(axis=0)\n",
    "      doc_vecs.append(mean)\n",
    "    array = np.array(doc_vecs)\n",
    "    return doc_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "start_time = time.time()\n",
    "print(\"Processing started at\", datetime.datetime.now())\n",
    "\n",
    "# Load dataset and apply dataset limit if not set to None.\n",
    "df = pd.read_csv(DATASET_FILE)\n",
    "print(df.shape)\n",
    "reviews = df['review'][:DATASET_LIMIT]\n",
    "sentiments = df['sentiment'][:DATASET_LIMIT]\n",
    "\n",
    "# Load EWE vector dictionary\n",
    "#ewe_vectors = load_ewe(EWE_FILE)\n",
    "\n",
    "# Build EWE pipeline with tokenizer\n",
    "#ewe = EWETransformerDocLevel(size=300, iter=50)\n",
    "#ewe_pipeline = Pipeline([\n",
    "#  (\"tokenize\", WordTokenizer()),\n",
    "#  (\"ewe\", ewe)\n",
    "#])\n",
    "\n",
    "# Build Word2Vec pipeline with tokenizer\n",
    "tokenize = WordTokenizer()\n",
    "w2v = W2VTransformerDocLevel(size=100, iter=50)\n",
    "w2v_pipeline = Pipeline([\n",
    "  (\"tokenize\", tokenize),\n",
    "  (\"w2v\", w2v)\n",
    "])\n",
    "\n",
    "# Build ngrams pipeline including unigrams and bigrams\n",
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "tf_idf = TfidfTransformer()\n",
    "ngram_tf_idf_pipeline = Pipeline([\n",
    "  (\"vect\", vect),\n",
    "  (\"tf_idf\", tf_idf)\n",
    "])\n",
    "\n",
    "# Build feature union of all desired pipelines. Comment out items to disable.\n",
    "features = FeatureUnion([\n",
    "  (\"ngram_tf_idf_pipeline\", ngram_tf_idf_pipeline),\n",
    "  #(\"ewe_pipeline\", ewe_pipeline),\n",
    "  (\"w2v_pipeline\", w2v_pipeline)\n",
    "])\n",
    "\n",
    "# Set up Linear SVC classifier, which seems good for sentiment analysis.\n",
    "classifier = LinearSVC(C=1.0, class_weight=\"balanced\", verbose=1)\n",
    "\n",
    "# Build main pipeline\n",
    "pipeline = Pipeline([\n",
    "  (\"features\", features),\n",
    "  (\"classifier\", classifier)\n",
    "])\n",
    "\n",
    "# Set up f1 scoring\n",
    "scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "# Train, test and score model on dataset\n",
    "scores = cross_val_score(pipeline, reviews, sentiments, scoring=scorer, cv=10)\n",
    "print(\"\\nAccuracy:\",scores.mean())\n",
    "\n",
    "# Stop timer\n",
    "end_time = time.time()\n",
    "time_elapsed = end_time - start_time\n",
    "print(\"Processing ended at\", datetime.datetime.now())\n",
    "print(time_elapsed, \"seconds elapsed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
