{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cove_ngrams.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdtO8XWxi1DY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8f1f084-1b61-45ed-9634-26409120b5b7"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "\n",
        "from gensim.sklearn_api import D2VTransformer, W2VTransformer\n",
        "from sklearn import metrics\n",
        "from sklearn.base import BaseEstimator, MetaEstimatorMixin, TransformerMixin\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.pipeline import FeatureUnion, Pipeline\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchtext\n",
        "from torchtext import data, datasets\n",
        "from torchtext.vocab import Vocab, GloVe, Vectors\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
        "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aVLBGjgmgCs"
      },
      "source": [
        "# CoVe Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRFKVUD5mgbQ"
      },
      "source": [
        "model_urls = {\n",
        "    'wmt-lstm' : 'https://s3.amazonaws.com/research.metamind.io/cove/wmtlstm-8f474287.pth'\n",
        "}\n",
        "\n",
        "MODEL_CACHE = '.torch'\n",
        "\n",
        "\n",
        "class MTLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, n_vocab=None, vectors=None, residual_embeddings=False, layer0=False, layer1=True, trainable=False, model_cache=MODEL_CACHE):\n",
        "        \"\"\"Initialize an MTLSTM. If layer0 and layer1 are True, they are concatenated along the last dimension so that layer0 outputs\n",
        "           contribute the first 600 entries and layer1 contributes the second 600 entries. If residual embeddings is also true, inputs\n",
        "           are also concatenated along the last dimension with any outputs such that they form the first 300 entries.\n",
        "         \n",
        "        Arguments:\n",
        "            n_vocab (int): If not None, initialize MTLSTM with an embedding matrix with n_vocab vectors\n",
        "            vectors (Float Tensor): If not None, initialize embedding matrix with specified vectors (These should be 300d CommonCrawl GloVe vectors)\n",
        "            residual_embedding (bool): If True, concatenate the input GloVe embeddings with contextualized word vectors as final output\n",
        "            layer0 (bool): If True, return the outputs of the first layer of the MTLSTM\n",
        "            layer1 (bool): If True, return the outputs of the second layer of the MTLSTM\n",
        "            trainable (bool): If True, do not detach outputs; i.e. train the MTLSTM (recommended to leave False)\n",
        "            model_cache (str): path to the model file for the MTLSTM to load pretrained weights (defaults to the best MTLSTM from (McCann et al. 2017) -- \n",
        "                               that MTLSTM was trained with 300d 840B GloVe on the WMT 2017 machine translation dataset.\n",
        "        \"\"\"\n",
        "        super(MTLSTM, self).__init__()\n",
        "        self.layer0 = layer0\n",
        "        self.layer1 = layer1\n",
        "        self.residual_embeddings = residual_embeddings\n",
        "        self.trainable = trainable\n",
        "        self.embed = False\n",
        "        if n_vocab is not None:\n",
        "            self.embed = True\n",
        "            self.vectors = nn.Embedding(n_vocab, 300)\n",
        "            if vectors is not None:\n",
        "                self.vectors.weight.data = vectors\n",
        "        state_dict = model_zoo.load_url(model_urls['wmt-lstm'], model_dir=model_cache)\n",
        "        if layer0:\n",
        "            layer0_dict = {k: v for k, v in state_dict.items() if 'l0' in k}\n",
        "            self.rnn0 = nn.LSTM(300, 300, num_layers=1, bidirectional=True, batch_first=True)\n",
        "            self.rnn0.load_state_dict(layer0_dict)\n",
        "            if layer1:\n",
        "                layer1_dict = {k.replace('l1', 'l0'): v for k, v in state_dict.items() if 'l1' in k}\n",
        "                self.rnn1 = nn.LSTM(600, 300, num_layers=1, bidirectional=True, batch_first=True)\n",
        "                self.rnn1.load_state_dict(layer1_dict)\n",
        "        elif layer1:\n",
        "            self.rnn1 = nn.LSTM(300, 300, num_layers=2, bidirectional=True, batch_first=True)\n",
        "            self.rnn1.load_state_dict(model_zoo.load_url(model_urls['wmt-lstm'], model_dir=model_cache))\n",
        "        else:\n",
        "            raise ValueError('At least one of layer0 and layer1 must be True.')\n",
        "         \n",
        "\n",
        "    def forward(self, inputs, lengths, hidden=None):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            inputs (Tensor): If MTLSTM handles embedding, a Long Tensor of size (batch_size, timesteps).\n",
        "                             Otherwise, a Float Tensor of size (batch_size, timesteps, features).\n",
        "            lengths (Long Tensor): lenghts of each sequence for handling padding\n",
        "            hidden (Float Tensor): initial hidden state of the LSTM\n",
        "        \"\"\"\n",
        "        if self.embed:\n",
        "            inputs = self.vectors(inputs)\n",
        "        if not isinstance(lengths, torch.Tensor):\n",
        "            lengths = torch.Tensor(lengths).long()\n",
        "            if inputs.is_cuda:\n",
        "                with torch.cuda.device_of(inputs):\n",
        "                    lengths = lengths.cuda(torch.cuda.current_device())\n",
        "        lens, indices = torch.sort(lengths, 0, True)\n",
        "        outputs = [inputs] if self.residual_embeddings else []\n",
        "        len_list = lens.tolist()\n",
        "        packed_inputs = pack(inputs[indices], len_list, batch_first=True)\n",
        "\n",
        "        if self.layer0:\n",
        "            outputs0, hidden_t0 = self.rnn0(packed_inputs, hidden)\n",
        "            unpacked_outputs0 = unpack(outputs0, batch_first=True)[0]\n",
        "            _, _indices = torch.sort(indices, 0)\n",
        "            unpacked_outputs0 = unpacked_outputs0[_indices]\n",
        "            outputs.append(unpacked_outputs0)\n",
        "            packed_inputs = outputs0\n",
        "        if self.layer1:\n",
        "            outputs1, hidden_t1 = self.rnn1(packed_inputs, hidden)\n",
        "            unpacked_outputs1 = unpack(outputs1, batch_first=True)[0]\n",
        "            _, _indices = torch.sort(indices, 0)\n",
        "            unpacked_outputs1 = unpacked_outputs1[_indices]\n",
        "            outputs.append(unpacked_outputs1)\n",
        "\n",
        "        outputs = torch.cat(outputs, 2)\n",
        "        return outputs if self.trainable else outputs.detach()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u82DYHe-OfmV"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQCSTHlv7gKC"
      },
      "source": [
        "# Load IMDb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHjvF3xkkpQG",
        "outputId": "b5600a62-d9a4-47ad-85fb-8216f8497a44"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbdoN_Haufbd"
      },
      "source": [
        "# Bring in IMDB data and make training/testing splits\n",
        "df = pd.read_csv('/content/drive/MyDrive/cs510_nlp/project/IMDB_Dataset.csv')\n",
        "\n",
        "# Shuffle data before training\n",
        "idx = np.arange(50000)\n",
        "np.random.shuffle(idx)\n",
        "train_idx = idx[:25000]\n",
        "test_idx = idx[25000:]\n",
        "\n",
        "# Make splits\n",
        "x_train = df.review[train_idx].to_list()\n",
        "y_train = [0 if s == 'negative' else 1 for s in df.sentiment[train_idx]]\n",
        "x_test = df.review[test_idx].to_list()\n",
        "y_test = [0 if s == 'negative' else 1 for s in df.sentiment[test_idx]]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AesMe-DKlAf1"
      },
      "source": [
        "# Transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTpYP292ZuYc"
      },
      "source": [
        "class WordTokenizer(BaseEstimator, MetaEstimatorMixin):\n",
        "  \"\"\"Tokenize input strings based on a simple word-boundary pattern.\"\"\"\n",
        "  \n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "    \n",
        "  def transform(self, X):\n",
        "    token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
        "    parser = lambda doc: token_pattern.findall(doc)\n",
        "    X = [parser(x) for x in X]\n",
        "    return X\n",
        "\n",
        "class W2VTransformerDocLevel(W2VTransformer):\n",
        "  \"\"\"Extend Gensim's Word2Vec sklearn-wrapper class to further transform\n",
        "  word-vectors into doc-vectors by averaging the words in each document.\"\"\"\n",
        "    \n",
        "  def __init__(self, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=1e-3, seed=1,\n",
        "    workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, cbow_mean=1, hashfxn=hash, iter=5, null_word=0,\n",
        "    trim_rule=None, sorted_vocab=1, batch_words=10000):\n",
        "    super().__init__(size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words)\n",
        "    \n",
        "  def transform(self, docs):      \n",
        "    doc_vecs = []\n",
        "    for doc in docs:\n",
        "      # for each document generate a word matrix\n",
        "      word_vectors_per_doc = []\n",
        "      for word in doc:\n",
        "        # handle out-of vocabulary words\n",
        "        if word in self.gensim_model.wv:\n",
        "          word_vectors_per_doc.append(self.gensim_model.wv[word])            \n",
        "      word_vectors_per_doc = np.array(word_vectors_per_doc)\n",
        "      # take the column-wise mean of this matrix and store\n",
        "      doc_vecs.append(word_vectors_per_doc.mean(axis=0))\n",
        "    return np.array(doc_vecs)\n",
        "\n",
        "\n",
        "class CoVeTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, tokenizer=WordTokenizer(), vectors='glove.6B.300d', device='cpu'):\n",
        "        self.tokenizer = tokenizer.transform\n",
        "        self.vectors = vectors\n",
        "        self.device = device\n",
        "        self.vocab = None\n",
        "        self.text_pipeline = lambda x: [self.vocab[token] for token in self.tokenizer(x)[0]]\n",
        "\n",
        "    \n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "\n",
        "    def transform(self, X):\n",
        "        print(f\"Generating CoVe vectors\")\n",
        "        if self.vocab is None:\n",
        "            self._build_vocab(X)\n",
        "\n",
        "        print(f\"Converting reviews to CoVe vectors\")\n",
        "        dataloader = DataLoader(X, batch_size=1, shuffle=False, collate_fn=self._collate_batch)\n",
        "        self.model = MTLSTM(n_vocab=len(self.vocab), vectors=self.vocab.vectors, residual_embeddings=True)\n",
        "        self.model.to(self.device)\n",
        "        cove_out = []\n",
        "        for i, x in enumerate(X):\n",
        "            if i % 1000 == 0:\n",
        "                print(f\"Transforming record {i}\")\n",
        "            text, lens = self._collate_batch([x])\n",
        "            vec = self.model(text, lens)\n",
        "            cove_out.append(vec.median(dim=1).values.squeeze(0))\n",
        "\n",
        "        return torch.stack(cove_out).cpu().numpy()\n",
        "\n",
        "\n",
        "    def _build_vocab(self, X):\n",
        "        print(f\"Building vocabulary\")\n",
        "        self.counter = Counter()\n",
        "        for tokens in self.tokenizer(X):\n",
        "            self.counter.update(tokens)\n",
        "        self.vocab = Vocab(self.counter, vectors=self.vectors)\n",
        "    \n",
        "    \n",
        "    def _collate_batch(self, batch):\n",
        "        text_list, len_list = [], []\n",
        "        for _text in batch:\n",
        "            processed_text = torch.tensor(self.text_pipeline([_text]), dtype=torch.int64)\n",
        "            text_list.append(processed_text)\n",
        "            len_list.append(processed_text.shape[0])\n",
        "        text_list = pad_sequence(text_list, batch_first=True)\n",
        "        len_list = torch.tensor(len_list).long()\n",
        "\n",
        "        return text_list.to(self.device), len_list.to(self.device)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WUcyqkbkUYg"
      },
      "source": [
        "# GloVe (6B) + CoVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yCKyz2jewPJ"
      },
      "source": [
        "pipeline = Pipeline([('feature_extraction', CoVeTransformer(device=device)), \n",
        "                     ('clf', LinearSVC(C=1.0, class_weight=\"balanced\"))])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJsm3TgZsVEZ",
        "outputId": "5aa9810a-0e3a-4e53-a5c6-a6ef437ebc41"
      },
      "source": [
        "# Train model\n",
        "pipeline.fit(x_train, y_train)\n",
        "\n",
        "# Make prediction\n",
        "predicted = pipeline.predict(x_test)\n",
        "\n",
        "# Check results\n",
        "results = metrics.classification_report(y_test, predicted)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transforming record 3000\n",
            "Transforming record 4000\n",
            "Transforming record 5000\n",
            "Transforming record 6000\n",
            "Transforming record 7000\n",
            "Transforming record 8000\n",
            "Transforming record 9000\n",
            "Transforming record 10000\n",
            "Transforming record 11000\n",
            "Transforming record 12000\n",
            "Transforming record 13000\n",
            "Transforming record 14000\n",
            "Transforming record 15000\n",
            "Transforming record 16000\n",
            "Transforming record 17000\n",
            "Transforming record 18000\n",
            "Transforming record 19000\n",
            "Transforming record 20000\n",
            "Transforming record 21000\n",
            "Transforming record 22000\n",
            "Transforming record 23000\n",
            "Transforming record 24000\n",
            "Generating CoVe vectors\n",
            "Converting reviews to CoVe vectors\n",
            "Transforming record 0\n",
            "Transforming record 1000\n",
            "Transforming record 2000\n",
            "Transforming record 3000\n",
            "Transforming record 4000\n",
            "Transforming record 5000\n",
            "Transforming record 6000\n",
            "Transforming record 7000\n",
            "Transforming record 8000\n",
            "Transforming record 9000\n",
            "Transforming record 10000\n",
            "Transforming record 11000\n",
            "Transforming record 12000\n",
            "Transforming record 13000\n",
            "Transforming record 14000\n",
            "Transforming record 15000\n",
            "Transforming record 16000\n",
            "Transforming record 17000\n",
            "Transforming record 18000\n",
            "Transforming record 19000\n",
            "Transforming record 20000\n",
            "Transforming record 21000\n",
            "Transforming record 22000\n",
            "Transforming record 23000\n",
            "Transforming record 24000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfDGBxKkkbg9",
        "outputId": "cd95a589-d1db-438d-9d66-ccd684ce99d8"
      },
      "source": [
        "print(results)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.81      0.81     12479\n",
            "           1       0.81      0.81      0.81     12521\n",
            "\n",
            "    accuracy                           0.81     25000\n",
            "   macro avg       0.81      0.81      0.81     25000\n",
            "weighted avg       0.81      0.81      0.81     25000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSl_lC8Qll5F"
      },
      "source": [
        "# GloVe (840B) + CoVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBxqXKLWlxXE"
      },
      "source": [
        "pipeline2 = Pipeline([('feature_extraction', CoVeTransformer(vectors='glove.840B.300d', device=device)), \n",
        "                     ('clf', LinearSVC(C=1.0, class_weight=\"balanced\"))])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_PisrjLlxXF",
        "outputId": "7bf5f7be-b7b9-437b-fea3-fd84615a70b6"
      },
      "source": [
        "# Train model\n",
        "pipeline2.fit(x_train, y_train)\n",
        "\n",
        "# Make prediction\n",
        "predicted2 = pipeline2.predict(x_test)\n",
        "\n",
        "# Check results\n",
        "results2 = metrics.classification_report(y_test, predicted2)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating CoVe vectors\n",
            "Building vocabulary\n",
            "Converting reviews to CoVe vectors\n",
            "Transforming record 0\n",
            "Transforming record 1000\n",
            "Transforming record 2000\n",
            "Transforming record 3000\n",
            "Transforming record 4000\n",
            "Transforming record 5000\n",
            "Transforming record 6000\n",
            "Transforming record 7000\n",
            "Transforming record 8000\n",
            "Transforming record 9000\n",
            "Transforming record 10000\n",
            "Transforming record 11000\n",
            "Transforming record 12000\n",
            "Transforming record 13000\n",
            "Transforming record 14000\n",
            "Transforming record 15000\n",
            "Transforming record 16000\n",
            "Transforming record 17000\n",
            "Transforming record 18000\n",
            "Transforming record 19000\n",
            "Transforming record 20000\n",
            "Transforming record 21000\n",
            "Transforming record 22000\n",
            "Transforming record 23000\n",
            "Transforming record 24000\n",
            "Generating CoVe vectors\n",
            "Converting reviews to CoVe vectors\n",
            "Transforming record 0\n",
            "Transforming record 1000\n",
            "Transforming record 2000\n",
            "Transforming record 3000\n",
            "Transforming record 4000\n",
            "Transforming record 5000\n",
            "Transforming record 6000\n",
            "Transforming record 7000\n",
            "Transforming record 8000\n",
            "Transforming record 9000\n",
            "Transforming record 10000\n",
            "Transforming record 11000\n",
            "Transforming record 12000\n",
            "Transforming record 13000\n",
            "Transforming record 14000\n",
            "Transforming record 15000\n",
            "Transforming record 16000\n",
            "Transforming record 17000\n",
            "Transforming record 18000\n",
            "Transforming record 19000\n",
            "Transforming record 20000\n",
            "Transforming record 21000\n",
            "Transforming record 22000\n",
            "Transforming record 23000\n",
            "Transforming record 24000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dy2hUre2lxXF",
        "outputId": "b7c7d647-5148-4e78-b00b-11d03bf08be4"
      },
      "source": [
        "print(results2)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85     12479\n",
            "           1       0.85      0.85      0.85     12521\n",
            "\n",
            "    accuracy                           0.85     25000\n",
            "   macro avg       0.85      0.85      0.85     25000\n",
            "weighted avg       0.85      0.85      0.85     25000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBBGE0JxlFRy"
      },
      "source": [
        "# Glove (6B) + CoVe + NGrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi0vKlIcnHe1"
      },
      "source": [
        "# Build ngrams pipeline\n",
        "vect = CountVectorizer(ngram_range=(1,2))\n",
        "tf_idf = TfidfTransformer()\n",
        "ngram_tf_idf_pipeline = Pipeline([\n",
        "  (\"vect\", vect),\n",
        "  (\"tf_idf\", tf_idf)\n",
        "])\n",
        "\n",
        "# Build GloVe -> CoVe pipeline\n",
        "glove_cove_pipeline = CoVeTransformer(device=device)\n",
        "\n",
        "# Build feature union of all pipelines\n",
        "features = FeatureUnion([\n",
        "  (\"ngram_tf_idf_pipeline\", ngram_tf_idf_pipeline),\n",
        "  (\"glove_cove_pipeline\", glove_cove_pipeline)\n",
        "])\n",
        "\n",
        "# Set up classifier\n",
        "classifier = LinearSVC(C=1.0, class_weight=\"balanced\")\n",
        "\n",
        "# Build main pipeline\n",
        "pipeline3 = Pipeline([\n",
        "     (\"features\", features),\n",
        "     (\"classifier\", classifier)\n",
        " ])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KF0E9DV1lN3c",
        "outputId": "3a79841e-a0d6-4643-b585-6ceadbe231d8"
      },
      "source": [
        "# Train model\n",
        "pipeline3.fit(x_train, y_train)\n",
        "\n",
        "# Make prediction\n",
        "predicted3 = pipeline3.predict(x_test)\n",
        "\n",
        "# Check results\n",
        "results3 = metrics.classification_report(y_test, predicted3)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating CoVe vectors\n",
            "Building vocabulary\n",
            "Converting reviews to CoVe vectors\n",
            "Transforming record 0\n",
            "Transforming record 1000\n",
            "Transforming record 2000\n",
            "Transforming record 3000\n",
            "Transforming record 4000\n",
            "Transforming record 5000\n",
            "Transforming record 6000\n",
            "Transforming record 7000\n",
            "Transforming record 8000\n",
            "Transforming record 9000\n",
            "Transforming record 10000\n",
            "Transforming record 11000\n",
            "Transforming record 12000\n",
            "Transforming record 13000\n",
            "Transforming record 14000\n",
            "Transforming record 15000\n",
            "Transforming record 16000\n",
            "Transforming record 17000\n",
            "Transforming record 18000\n",
            "Transforming record 19000\n",
            "Transforming record 20000\n",
            "Transforming record 21000\n",
            "Transforming record 22000\n",
            "Transforming record 23000\n",
            "Transforming record 24000\n",
            "Generating CoVe vectors\n",
            "Converting reviews to CoVe vectors\n",
            "Transforming record 0\n",
            "Transforming record 1000\n",
            "Transforming record 2000\n",
            "Transforming record 3000\n",
            "Transforming record 4000\n",
            "Transforming record 5000\n",
            "Transforming record 6000\n",
            "Transforming record 7000\n",
            "Transforming record 8000\n",
            "Transforming record 9000\n",
            "Transforming record 10000\n",
            "Transforming record 11000\n",
            "Transforming record 12000\n",
            "Transforming record 13000\n",
            "Transforming record 14000\n",
            "Transforming record 15000\n",
            "Transforming record 16000\n",
            "Transforming record 17000\n",
            "Transforming record 18000\n",
            "Transforming record 19000\n",
            "Transforming record 20000\n",
            "Transforming record 21000\n",
            "Transforming record 22000\n",
            "Transforming record 23000\n",
            "Transforming record 24000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByodSH3jlTX3",
        "outputId": "55dd2c5d-b910-43d1-9db9-31e6b0e126b9"
      },
      "source": [
        "print(results3)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.89      0.90     12479\n",
            "           1       0.89      0.92      0.90     12521\n",
            "\n",
            "    accuracy                           0.90     25000\n",
            "   macro avg       0.90      0.90      0.90     25000\n",
            "weighted avg       0.90      0.90      0.90     25000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmwxV93g0s4v"
      },
      "source": [
        "# Glove (840B) + CoVe + NGrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrUePnke0s4z"
      },
      "source": [
        "# Build ngrams pipeline\n",
        "vect = CountVectorizer(ngram_range=(1,2))\n",
        "tf_idf = TfidfTransformer()\n",
        "ngram_tf_idf_pipeline = Pipeline([\n",
        "  (\"vect\", vect),\n",
        "  (\"tf_idf\", tf_idf)\n",
        "])\n",
        "\n",
        "# Build GloVe -> CoVe pipeline\n",
        "glove_cove_pipeline = CoVeTransformer(vectors='glove.840B.300d', device=device)\n",
        "\n",
        "# Build feature union of all pipelines\n",
        "features = FeatureUnion([\n",
        "  (\"ngram_tf_idf_pipeline\", ngram_tf_idf_pipeline),\n",
        "  (\"glove_cove_pipeline\", glove_cove_pipeline)\n",
        "])\n",
        "\n",
        "# Set up classifier\n",
        "classifier = LinearSVC(C=1.0, class_weight=\"balanced\")\n",
        "\n",
        "# Build main pipeline\n",
        "pipeline4 = Pipeline([\n",
        "     (\"features\", features),\n",
        "     (\"classifier\", classifier)\n",
        " ])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p77YBJVx0s4z",
        "outputId": "2ebdb061-4087-447a-b8af-3a69c5d36161"
      },
      "source": [
        "# Train model\n",
        "pipeline4.fit(x_train, y_train)\n",
        "\n",
        "# Make prediction\n",
        "predicted4 = pipeline4.predict(x_test)\n",
        "\n",
        "# Check results\n",
        "results4 = metrics.classification_report(y_test, predicted4)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating CoVe vectors\n",
            "Converting reviews to CoVe vectors\n",
            "Transforming record 0\n",
            "Transforming record 1000\n",
            "Transforming record 2000\n",
            "Transforming record 3000\n",
            "Transforming record 4000\n",
            "Transforming record 5000\n",
            "Transforming record 6000\n",
            "Transforming record 7000\n",
            "Transforming record 8000\n",
            "Transforming record 9000\n",
            "Transforming record 10000\n",
            "Transforming record 11000\n",
            "Transforming record 12000\n",
            "Transforming record 13000\n",
            "Transforming record 14000\n",
            "Transforming record 15000\n",
            "Transforming record 16000\n",
            "Transforming record 17000\n",
            "Transforming record 18000\n",
            "Transforming record 19000\n",
            "Transforming record 20000\n",
            "Transforming record 21000\n",
            "Transforming record 22000\n",
            "Transforming record 23000\n",
            "Transforming record 24000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvL_HJKK0s40",
        "outputId": "11b35b55-f782-449f-f51c-5b3b97847541"
      },
      "source": [
        "print(results4)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.90      0.91     12479\n",
            "           1       0.90      0.91      0.91     12521\n",
            "\n",
            "    accuracy                           0.91     25000\n",
            "   macro avg       0.91      0.91      0.91     25000\n",
            "weighted avg       0.91      0.91      0.91     25000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}